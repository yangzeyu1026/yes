{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOuRVpAd0+V2TEPs+wQzdXt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yangzeyu1026/yes/blob/master/beit2_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A0AmfoaWPlGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the shared drive, where the beit2 model is stored."
      ],
      "metadata": {
        "id": "aNha5eX1OHHB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKbEsDRjVQ25",
        "outputId": "4a386cce-d367-4ae5-de04-68c469a0ba00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import runtime\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this line in the first time to enter the directory of beit2."
      ],
      "metadata": {
        "id": "DnUAb_mNNRtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gdrive/Shareddrives/Vision\\ X/unilm/beit2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIZcAIqpVfKX",
        "outputId": "d4b64f5d-b751-4947-8651-9ddf3fb061ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/Shareddrives/Vision X/unilm/beit2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required packages specified in \"requirement.txt\"."
      ],
      "metadata": {
        "id": "RxhUeNDmN0PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmsYyKyLN23T",
        "outputId": "226beb84-3760-4f45-bd34-7848bbd54074"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 16 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2\n",
            "  Downloading torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 52.6 MB/s \n",
            "\u001b[?25hCollecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 72.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (7.1.2)\n",
            "Collecting blobfile\n",
            "  Downloading blobfile-2.0.0-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting mypy\n",
            "  Downloading mypy-0.991-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.5 MB 66.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.21.6)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (2.23.0)\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 547 kB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 72.5 MB/s \n",
            "\u001b[?25hCollecting deepspeed==0.4.0\n",
            "  Downloading deepspeed-0.4.0.tar.gz (444 kB)\n",
            "\u001b[K     |████████████████████████████████| 444 kB 66.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (1.7.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->-r requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.4.0->-r requirements.txt (line 12)) (4.64.1)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-1.8-py2.py3-none-any.whl (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 66.6 MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 66.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.4.0->-r requirements.txt (line 12)) (5.4.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.4.0->-r requirements.txt (line 12)) (21.3)\n",
            "Collecting triton\n",
            "  Downloading triton-1.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.2 MB 128 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r requirements.txt (line 11)) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r requirements.txt (line 11)) (3.19.6)\n",
            "Requirement already satisfied: filelock~=3.0 in /usr/local/lib/python3.7/dist-packages (from blobfile->-r requirements.txt (line 5)) (3.8.0)\n",
            "Collecting pycryptodomex~=3.8\n",
            "  Downloading pycryptodomex-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 49.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml~=4.9 in /usr/local/lib/python3.7/dist-packages (from blobfile->-r requirements.txt (line 5)) (4.9.1)\n",
            "Collecting urllib3~=1.25\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 72.7 MB/s \n",
            "\u001b[?25hCollecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Collecting typed-ast<2,>=1.4.0\n",
            "  Downloading typed_ast-1.5.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |████████████████████████████████| 843 kB 61.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mypy->-r requirements.txt (line 6)) (2.0.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 8)) (9.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 8)) (57.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 8)) (1.11.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 8)) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 8)) (22.1.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 9)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 9)) (2.10)\n",
            "Collecting urllib3~=1.25\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 72.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 9)) (2022.9.24)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->deepspeed==0.4.0->-r requirements.txt (line 12)) (3.0.9)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.4.0-py3-none-any.whl size=447185 sha256=02966a05c2fed7619ce86fd458c234b0a0961fb44d6951aee4f47a1688b0ea1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/23/8c/11c9079fc05f26d449903b33084413ec64d7c63f32203464bf\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: torch, urllib3, typed-ast, triton, torchvision, tensorboardX, pycryptodomex, ninja, mypy-extensions, timm, mypy, einops, deepspeed, blobfile\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.1+cu113\n",
            "    Uninstalling torchvision-0.13.1+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed blobfile-2.0.0 deepspeed-0.4.0 einops-0.6.0 mypy-0.991 mypy-extensions-0.4.3 ninja-1.11.1 pycryptodomex-3.15.0 tensorboardX-1.8 timm-0.4.12 torch-1.7.1 torchvision-0.8.2 triton-1.1.1 typed-ast-1.5.4 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install other packages."
      ],
      "metadata": {
        "id": "0qeRwuGhRUzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sfX2ak-RXC4",
        "outputId": "77b338ef-7e9a-499a-8576-1b1d4ef31428"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test whether Cuda is available."
      ],
      "metadata": {
        "id": "W5gYRm_ZT3a2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "device = torch.device('cuda:0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdmy6NLxT8Ch",
        "outputId": "283aae13-abad-414b-eccf-aeddb2ddd774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test python file cuda availability."
      ],
      "metadata": {
        "id": "dFfvwz2Dqe4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python cuda_test.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZNVaY4mqiCR",
        "outputId": "9a1bc8d8-e34b-4473-9be0-a893d4279322"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run test_get_code, which is the author's testing code, aiming to help us check whether the set works."
      ],
      "metadata": {
        "id": "c79MG0iqtU_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_get_code.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4ALkOtStEDi",
        "outputId": "37f4d214-9598-41d0-d097-d12a9ae23715"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image transforms: Compose(\n",
            "    Resize(size=256, interpolation=PIL.Image.BICUBIC)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            ")\n",
            "{}\n",
            "Final encoder config {'img_size': 224, 'patch_size': 16, 'in_chans': 3, 'num_classes': 0, 'embed_dim': 768, 'depth': 12, 'num_heads': 12, 'mlp_ratio': 4.0, 'qkv_bias': True, 'qk_scale': None, 'drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0, 'norm_layer': functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06), 'init_values': 0.0, 'use_abs_pos_emb': True, 'use_rel_pos_bias': False, 'use_shared_rel_pos_bias': False, 'use_mean_pooling': True, 'init_scale': 0.001}\n",
            "Final decoder config {'img_size': 14, 'patch_size': 1, 'in_chans': 32, 'num_classes': 0, 'embed_dim': 768, 'depth': 1, 'num_heads': 12, 'mlp_ratio': 4.0, 'qkv_bias': True, 'qk_scale': None, 'drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0, 'norm_layer': functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06), 'init_values': 0.0, 'use_abs_pos_emb': True, 'use_rel_pos_bias': False, 'use_shared_rel_pos_bias': False, 'use_mean_pooling': True, 'init_scale': 0.001}\n",
            "process type for VQKD: default\n",
            "Downloading: \"https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/vqkd_encoder_base_decoder_1x768x12_clip-d93179da.pth\" to /root/.cache/torch/hub/checkpoints/vqkd_encoder_base_decoder_1x768x12_clip-d93179da.pth\n",
            "100% 363M/363M [00:46<00:00, 8.11MB/s]\n",
            "tensor([[3812, 7466, 1913, 1913, 1903, 1913, 1903, 1913, 3812, 7820, 6337, 2189,\n",
            "         7466, 7466, 2492, 3743, 5268, 3481, 5268, 4987,  445, 8009, 3501, 5268,\n",
            "         7820, 7831, 4816, 2189, 7549, 7549, 5548, 4987,  445, 4198,  445, 5216,\n",
            "         4987, 5268, 3278, 5203, 6337, 1799,  847, 6454, 4527, 5302, 8009, 3743,\n",
            "         5216, 4678, 3743, 4858, 5203, 4816, 7831, 2189, 7549, 5386, 6628, 5004,\n",
            "         2779, 7131, 7131, 7131, 4928, 3743,  119,  445, 1903, 7466, 4527, 5386,\n",
            "         5398, 5704, 2104, 5398, 2779, 7258, 7989,  624, 7131, 1186, 5216, 7466,\n",
            "         8015, 5004,  452, 7243, 3145, 6690, 7017, 2104, 5398, 4198, 7989, 7131,\n",
            "         3717, 7466,  580, 5004, 5004, 6202, 6202, 6202, 1826, 7521, 1473, 5722,\n",
            "         2486, 5663, 4928, 3941,  580, 5548, 7983, 7983, 7983, 2104, 5004, 2063,\n",
            "         2637, 1822, 3100, 3100, 1405, 1637, 8187, 5433, 2779, 5398, 5004, 5004,\n",
            "         1107, 3469, 3469, 5302, 2590, 6381, 3100, 4194, 3717,  356, 7131, 7688,\n",
            "         5104, 3081, 3812, 3950, 1186, 7131, 7131, 3717, 4399, 1186, 2221, 6501,\n",
            "         7131, 5433, 3014, 3950, 3278, 2812, 7131, 1186, 7036, 6947, 7036, 4648,\n",
            "         2812, 7131, 3014, 5295, 7266, 5180, 4123, 3792, 4648, 8009, 4648, 4816,\n",
            "         1511, 7036,  375, 2221, 5813, 5698,  168, 7131, 3792, 5698, 5698, 2667,\n",
            "         5698, 4648, 4171, 6501]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_class_finetuning.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXEWvmxdvNOs",
        "outputId": "78d0f111-1f28-4d6b-939e-04e2e6fd8967"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Namespace(aa='rand-m9-mstd0.5-inc1', abs_pos_emb=False, attn_drop_rate=0.0, auto_resume=True, batch_size=64, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=0, cutmix_minmax=None, data_path='/datasets01/imagenet_full_size/061417/', data_set='IMNET', device='cuda', disable_eval_during_finetuning=False, disable_weight_decay_on_rel_pos_bias=False, dist_eval=False, dist_on_itp=False, dist_url='env://', distributed=False, drop=0.0, drop_path=0.1, enable_deepspeed=False, epochs=30, eval=False, eval_data_path=None, finetune='', image_folder_class_index_file=None, imagenet_default_mean_and_std=False, init_scale=0.001, input_size=224, layer_decay=0.9, layer_scale_init_value=0.1, load_tar=False, local_rank=-1, log_dir=None, lr=0.0005, min_lr=1e-06, mixup=0, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='deit_base_patch16_224', model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_filter_name='', model_key='model|module', model_prefix='', momentum=0.9, nb_classes=0, num_workers=10, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', pin_mem=True, qkv_bias=True, recount=1, rel_pos_bias=True, remode='pixel', reprob=0.25, resplit=False, resume='', robust_test=None, save_ckpt=True, save_ckpt_freq=5, seed=0, smoothing=0.1, start_epoch=0, train_interpolation='bicubic', update_freq=1, use_mean_pooling=True, warmup_epochs=5, warmup_lr=1e-06, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, world_size=1)\n",
            "Transform = \n",
            "RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BICUBIC)\n",
            "RandomHorizontalFlip(p=0.5)\n",
            "<timm.data.auto_augment.RandAugment object at 0x7f2cd7db4490>\n",
            "ToTensor()\n",
            "Normalize(mean=tensor([0.5000, 0.5000, 0.5000]), std=tensor([0.5000, 0.5000, 0.5000]))\n",
            "<timm.data.random_erasing.RandomErasing object at 0x7f2cd7db4450>\n",
            "---------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"run_class_finetuning.py\", line 628, in <module>\n",
            "    main(opts, ds_init)\n",
            "  File \"run_class_finetuning.py\", line 267, in main\n",
            "    dataset_train, args.nb_classes = build_dataset(is_train=True, args=args)\n",
            "  File \"/content/gdrive/Shareddrives/Vision X/unilm/beit2/datasets.py\", line 143, in build_dataset\n",
            "    dataset = datasets.ImageFolder(root, transform=transform)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\", line 229, in __init__\n",
            "    is_valid_file=is_valid_file)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\", line 108, in __init__\n",
            "    classes, class_to_idx = self._find_classes(self.root)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\", line 137, in _find_classes\n",
            "    classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/datasets01/imagenet_full_size/061417/train'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_vqkd_training.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKOY8M-zriKr",
        "outputId": "b2e3057a-913a-4a5a-b1ce-4e9989cf28d5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "Namespace(auto_resume=True, batch_size=64, calculate_codebook_usage=False, clip_grad=None, codebook_emd_dim=32, codebook_n_emd=8192, color_jitter=0.0, data_path='/datasets01/imagenet_full_size/061417/', data_set='image_folder', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, ema_decay=0.99, epochs=100, eval=False, eval_data_path='', imagenet_default_mean_and_std=False, input_size=224, local_rank=-1, log_dir=None, lr=5e-05, min_crop_scale=0.08, min_lr=1e-05, model='vqkd_encoder_base_decoder_3x768x12_clip', num_workers=10, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', pin_mem=True, process_type='default', quantize_kmeans_init=False, rec_loss_type='cosine', resume='', save_ckpt_freq=20, seed=0, start_epoch=0, teacher_input_size=224, teacher_model_type='clip', train_interpolation='bicubic', warmup_epochs=5, warmup_lr=1e-06, warmup_steps=-1, weight_decay=0.0001, weight_decay_end=None, world_size=1)\n",
            "{'teacher_input_size': 224}\n",
            "Final encoder config {'img_size': 224, 'patch_size': 16, 'in_chans': 3, 'num_classes': 0, 'embed_dim': 768, 'depth': 12, 'num_heads': 12, 'mlp_ratio': 4.0, 'qkv_bias': True, 'qk_scale': None, 'drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0, 'norm_layer': functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06), 'init_values': 0.0, 'use_abs_pos_emb': True, 'use_rel_pos_bias': False, 'use_shared_rel_pos_bias': False, 'use_mean_pooling': True, 'init_scale': 0.001}\n",
            "Final decoder config {'img_size': 14, 'patch_size': 1, 'in_chans': 32, 'num_classes': 0, 'embed_dim': 768, 'depth': 3, 'num_heads': 12, 'mlp_ratio': 4.0, 'qkv_bias': True, 'qk_scale': None, 'drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0, 'norm_layer': functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06), 'init_values': 0.0, 'use_abs_pos_emb': True, 'use_rel_pos_bias': False, 'use_shared_rel_pos_bias': False, 'use_mean_pooling': True, 'init_scale': 0.001}\n",
            "100%|███████████████████████████████████████| 335M/335M [00:04<00:00, 86.6MiB/s]\n",
            "process type for VQKD: default\n",
            "Train Data Aug: Compose(\n",
            "    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BICUBIC)\n",
            "    RandomHorizontalFlip(p=0.5)\n",
            "    ToTensor()\n",
            ")\n",
            "Traceback (most recent call last):\n",
            "  File \"run_vqkd_training.py\", line 340, in <module>\n",
            "    main(opts)\n",
            "  File \"run_vqkd_training.py\", line 182, in main\n",
            "    dataset_train = build_vqkd_dataset(is_train=True, args=args)\n",
            "  File \"/content/gdrive/Shareddrives/Vision X/unilm/beit2/datasets.py\", line 111, in build_vqkd_dataset\n",
            "    return ImageFolder(args.data_path, transform=transform)\n",
            "  File \"/content/gdrive/Shareddrives/Vision X/unilm/beit2/dataset_folder.py\", line 284, in __init__\n",
            "    is_valid_file=is_valid_file, index_file=index_file)\n",
            "  File \"/content/gdrive/Shareddrives/Vision X/unilm/beit2/dataset_folder.py\", line 121, in __init__\n",
            "    classes, class_to_idx = self._find_classes(self.root)\n",
            "  File \"/content/gdrive/Shareddrives/Vision X/unilm/beit2/dataset_folder.py\", line 172, in _find_classes\n",
            "    classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/datasets01/imagenet_full_size/061417/'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy from the file \"test_get_code.py\""
      ],
      "metadata": {
        "id": "gb_9GPdtM_MR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------\n",
        "# BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers (https://arxiv.org/abs/2208.06366)\n",
        "# Github source: https://github.com/microsoft/unilm/tree/master/beitv2\n",
        "# Copyright (c) 2022 Microsoft\n",
        "# Licensed under The MIT License [see LICENSE for details]\n",
        "# By Zhiliang Peng\n",
        "# Based on BEiT, timm, DeiT and DINO code bases\n",
        "# https://github.com/microsoft/unilm/tree/master/beit\n",
        "# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n",
        "# https://github.com/facebookresearch/deit/\n",
        "# https://github.com/facebookresearch/dino\n",
        "# --------------------------------------------------------'\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as pth_transforms\n",
        "from timm.models import create_model\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import utils\n",
        "import modeling_vqkd \n",
        "\n",
        "# def get_code(args):\n",
        "#     # ============ preparing data ... ============\n",
        "#     transform = pth_transforms.Compose([\n",
        "#         pth_transforms.Resize(256, interpolation=3),\n",
        "#         pth_transforms.CenterCrop(224),\n",
        "#         pth_transforms.ToTensor(),\n",
        "#         # pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # Normalize in pre-process of vqkd\n",
        "#     ])\n",
        "#     print(f\"Image transforms: {transform}\")\n",
        "\n",
        "#     images = transform(Image.open(args.img_path)).unsqueeze(0)\n",
        "\n",
        "#     # ============ building network ... ============\n",
        "#     model = create_model(\n",
        "#             args.model,\n",
        "#             pretrained=True,\n",
        "#             pretrained_weight=args.pretrained_weights,\n",
        "#             as_tokenzer=True,\n",
        "#         ).eval()\n",
        "\n",
        "#     input_ids = model.get_codebook_indices(images)\n",
        "#     print(input_ids)\n",
        "\n",
        "def get_code():\n",
        "    # ============ preparing data ... ============\n",
        "    transform = pth_transforms.Compose([\n",
        "        pth_transforms.Resize(256, interpolation=3),\n",
        "        pth_transforms.CenterCrop(224),\n",
        "        pth_transforms.ToTensor(),\n",
        "        # pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # Normalize in pre-process of vqkd\n",
        "    ])\n",
        "    print(f\"Image transforms: {transform}\")\n",
        "\n",
        "    images = transform(Image.open('demo/ILSVRC2012_val_00031649.JPEG')).unsqueeze(0)\n",
        "\n",
        "    # ============ building network ... ============\n",
        "    model = create_model(\n",
        "            'vqkd_encoder_base_decoder_1x768x12_clip',\n",
        "            pretrained=True,\n",
        "            pretrained_weight='https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/vqkd_encoder_base_decoder_1x768x12_clip-d93179da.pth',\n",
        "            as_tokenzer=True,\n",
        "        ).eval()\n",
        "\n",
        "    input_ids = model.get_codebook_indices(images)\n",
        "    print(input_ids)\n",
        "    # print(input_ids.is_cuda)\n",
        "    input_ids_cuda = input_ids.cuda()\n",
        "    print(input_ids_cuda)\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     parser = argparse.ArgumentParser('Get code for VQ-KD')\n",
        "#     parser.add_argument('--model', default='vqkd_encoder_base_decoder_1x768x12_clip', type=str, help=\"model\")\n",
        "#     parser.add_argument('--pretrained_weights', \n",
        "#                         default='https://conversationhub.blob.core.windows.net/beit-share-public/beitv2/vqkd_encoder_base_decoder_1x768x12_clip-d93179da.pth', \n",
        "#                         type=str, help=\"Path to pretrained weights to evaluate.\")\n",
        "#     parser.add_argument('--img_path', default='demo/ILSVRC2012_val_00031649.JPEG', type=str, help=\"image path.\")\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     get_code(args)\n",
        "# get_code()\n",
        "\n",
        "get_code()\n",
        "    \n",
        "    \n",
        "# tensor([[3812, 7466, 1913, 1913, 1903, 1913, 1903, 1913, 3812, 7820, 6337, 2189,\n",
        "#          7466, 7466, 2492, 3743, 5268, 3481, 5268, 4987,  445, 8009, 3501, 5268,\n",
        "#          7820, 7831, 4816, 2189, 7549, 7549, 5548, 4987,  445, 4198,  445, 5216,\n",
        "#          4987, 5268, 3278, 5203, 6337, 1799,  847, 6454, 4527, 5302, 8009, 3743,\n",
        "#          5216, 4678, 3743, 4858, 5203, 4816, 7831, 2189, 7549, 5386, 6628, 5004,\n",
        "#          2779, 7131, 7131, 7131, 4928, 3743,  119,  445, 1903, 7466, 4527, 5386,\n",
        "#          5398, 5704, 2104, 5398, 2779, 7258, 7989,  624, 7131, 1186, 5216, 7466,\n",
        "#          8015, 5004,  452, 7243, 3145, 6690, 7017, 2104, 5398, 4198, 7989, 7131,\n",
        "#          3717, 7466,  580, 5004, 5004, 6202, 6202, 6202, 1826, 7521, 1473, 5722,\n",
        "#          2486, 5663, 4928, 3941,  580, 5548, 7983, 7983, 7983, 2104, 5004, 2063,\n",
        "#          2637, 1822, 3100, 3100, 1405, 1637, 8187, 5433, 2779, 5398, 5004, 5004,\n",
        "#          1107, 3469, 3469, 5302, 2590, 6381, 3100, 4194, 3717,  356, 7131, 7688,\n",
        "#          5104, 3081, 3812, 3950, 1186, 7131, 7131, 3717, 4399, 1186, 2221, 6501,\n",
        "#          7131, 5433, 3014, 3950, 3278, 2812, 7131, 1186, 7036, 6947, 7036, 4648,\n",
        "#          2812, 7131, 3014, 5295, 7266, 5180, 4123, 3792, 4648, 8009, 4648, 4816,\n",
        "#          1511, 7036,  375, 2221, 5813, 5698,  168, 7131, 3792, 5698, 5698, 2667,\n",
        "#          5698, 4648, 4171, 6501]], device='cuda:0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLp6L2-yNEo3",
        "outputId": "ae614c11-293d-4f01-c017-874e3b4024d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image transforms: Compose(\n",
            "    Resize(size=256, interpolation=PIL.Image.BICUBIC)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    ToTensor()\n",
            ")\n",
            "{}\n",
            "Final encoder config {'img_size': 224, 'patch_size': 16, 'in_chans': 3, 'num_classes': 0, 'embed_dim': 768, 'depth': 12, 'num_heads': 12, 'mlp_ratio': 4.0, 'qkv_bias': True, 'qk_scale': None, 'drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0, 'norm_layer': functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06), 'init_values': 0.0, 'use_abs_pos_emb': True, 'use_rel_pos_bias': False, 'use_shared_rel_pos_bias': False, 'use_mean_pooling': True, 'init_scale': 0.001}\n",
            "Final decoder config {'img_size': 14, 'patch_size': 1, 'in_chans': 32, 'num_classes': 0, 'embed_dim': 768, 'depth': 1, 'num_heads': 12, 'mlp_ratio': 4.0, 'qkv_bias': True, 'qk_scale': None, 'drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0, 'norm_layer': functools.partial(<class 'torch.nn.modules.normalization.LayerNorm'>, eps=1e-06), 'init_values': 0.0, 'use_abs_pos_emb': True, 'use_rel_pos_bias': False, 'use_shared_rel_pos_bias': False, 'use_mean_pooling': True, 'init_scale': 0.001}\n",
            "process type for VQKD: default\n",
            "tensor([[3812, 7466, 1913, 1913, 1903, 1913, 1903, 1913, 3812, 7820, 6337, 2189,\n",
            "         7466, 7466, 2492, 3743, 5268, 3481, 5268, 4987,  445, 8009, 3501, 5268,\n",
            "         7820, 7831, 4816, 2189, 7549, 7549, 5548, 4987,  445, 4198,  445, 5216,\n",
            "         4987, 5268, 3278, 5203, 6337, 1799,  847, 6454, 4527, 5302, 8009, 3743,\n",
            "         5216, 4678, 3743, 4858, 5203, 4816, 7831, 2189, 7549, 5386, 6628, 5004,\n",
            "         2779, 7131, 7131, 7131, 4928, 3743,  119,  445, 1903, 7466, 4527, 5386,\n",
            "         5398, 5704, 2104, 5398, 2779, 7258, 7989,  624, 7131, 1186, 5216, 7466,\n",
            "         8015, 5004,  452, 7243, 3145, 6690, 7017, 2104, 5398, 4198, 7989, 7131,\n",
            "         3717, 7466,  580, 5004, 5004, 6202, 6202, 6202, 1826, 7521, 1473, 5722,\n",
            "         2486, 5663, 4928, 3941,  580, 5548, 7983, 7983, 7983, 2104, 5004, 2063,\n",
            "         2637, 1822, 3100, 3100, 1405, 1637, 8187, 5433, 2779, 5398, 5004, 5004,\n",
            "         1107, 3469, 3469, 5302, 2590, 6381, 3100, 4194, 3717,  356, 7131, 7688,\n",
            "         5104, 3081, 3812, 3950, 1186, 7131, 7131, 3717, 4399, 1186, 2221, 6501,\n",
            "         7131, 5433, 3014, 3950, 3278, 2812, 7131, 1186, 7036, 6947, 7036, 4648,\n",
            "         2812, 7131, 3014, 5295, 7266, 5180, 4123, 3792, 4648, 8009, 4648, 4816,\n",
            "         1511, 7036,  375, 2221, 5813, 5698,  168, 7131, 3792, 5698, 5698, 2667,\n",
            "         5698, 4648, 4171, 6501]])\n",
            "tensor([[3812, 7466, 1913, 1913, 1903, 1913, 1903, 1913, 3812, 7820, 6337, 2189,\n",
            "         7466, 7466, 2492, 3743, 5268, 3481, 5268, 4987,  445, 8009, 3501, 5268,\n",
            "         7820, 7831, 4816, 2189, 7549, 7549, 5548, 4987,  445, 4198,  445, 5216,\n",
            "         4987, 5268, 3278, 5203, 6337, 1799,  847, 6454, 4527, 5302, 8009, 3743,\n",
            "         5216, 4678, 3743, 4858, 5203, 4816, 7831, 2189, 7549, 5386, 6628, 5004,\n",
            "         2779, 7131, 7131, 7131, 4928, 3743,  119,  445, 1903, 7466, 4527, 5386,\n",
            "         5398, 5704, 2104, 5398, 2779, 7258, 7989,  624, 7131, 1186, 5216, 7466,\n",
            "         8015, 5004,  452, 7243, 3145, 6690, 7017, 2104, 5398, 4198, 7989, 7131,\n",
            "         3717, 7466,  580, 5004, 5004, 6202, 6202, 6202, 1826, 7521, 1473, 5722,\n",
            "         2486, 5663, 4928, 3941,  580, 5548, 7983, 7983, 7983, 2104, 5004, 2063,\n",
            "         2637, 1822, 3100, 3100, 1405, 1637, 8187, 5433, 2779, 5398, 5004, 5004,\n",
            "         1107, 3469, 3469, 5302, 2590, 6381, 3100, 4194, 3717,  356, 7131, 7688,\n",
            "         5104, 3081, 3812, 3950, 1186, 7131, 7131, 3717, 4399, 1186, 2221, 6501,\n",
            "         7131, 5433, 3014, 3950, 3278, 2812, 7131, 1186, 7036, 6947, 7036, 4648,\n",
            "         2812, 7131, 3014, 5295, 7266, 5180, 4123, 3792, 4648, 8009, 4648, 4816,\n",
            "         1511, 7036,  375, 2221, 5813, 5698,  168, 7131, 3792, 5698, 5698, 2667,\n",
            "         5698, 4648, 4171, 6501]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cZ_VJnyWpoMG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2I3ZB6KBpovK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}